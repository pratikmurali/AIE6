{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCk2Rx4cjlYF"
      },
      "source": [
        "# Synthetic Data Generation Using RAGAS - RAG Evaluation with LangSmith\n",
        "\n",
        "In the following notebook we'll explore a use-case for RAGAS' synthetic testset generation workflow!\n",
        "\n",
        "\n",
        "\n",
        "- 🤝 BREAKOUT ROOM #1\n",
        "  1. Use RAGAS to Generate Synthetic Data\n",
        "\n",
        "- 🤝 BREAKOUT ROOM #2\n",
        "  1. Load them into a LangSmith Dataset\n",
        "  2. Evaluate our RAG chain against the synthetic test data\n",
        "  3. Make changes to our pipeline\n",
        "  4. Evaluate the modified pipeline\n",
        "\n",
        "SDG is a critical piece of the puzzle, especially for early iteration! Without it, it would not be nearly as easy to get high quality early signal for our application's performance.\n",
        "\n",
        "Let's dive in!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bG2ta-B478G"
      },
      "source": [
        "# 🤝 BREAKOUT ROOM #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VUI7vF_kbv9"
      },
      "source": [
        "## Task 1: Dependencies and API Keys\n",
        "\n",
        "We'll need to install a number of API keys and dependencies, since we'll be leveraging a number of great technologies for this pipeline!\n",
        "\n",
        "1. OpenAI's endpoints to handle the Synthetic Data Generation\n",
        "2. OpenAI's Endpoints for our RAG pipeline and LangSmith evaluation\n",
        "3. QDrant as our vectorstore\n",
        "4. LangSmith for our evaluation coordinator!\n",
        "\n",
        "Let's install and provide all the required information below!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dependencies and API Keys:\n",
        "\n",
        "> NOTE: DO NOT RUN THESE CELLS IF YOU ARE RUNNING THIS NOTEBOOK LOCALLY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/175.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.7/175.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/71.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m160.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "#!pip install -qU ragas==0.2.10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install -qU langchain-community==0.3.14 langchain-openai==0.2.14 unstructured==0.16.12 langgraph==0.2.61 langchain-qdrant==0.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NLTK Import\n",
        "\n",
        "To prevent errors that may occur based on OS - we'll import NLTK and download the needed packages to ensure correct handling of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/pratikmurali/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/pratikmurali/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangChain API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll also want to set a project name to make things easier for ourselves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIM - SDG - {uuid4().hex[0:8]}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OpenAI's API Key!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating Synthetic Test Data\n",
        "\n",
        "We wil be using Ragas to build out a set of synthetic test questions, references, and reference contexts. This is useful because it will allow us to find out how our system is performing.\n",
        "\n",
        "> NOTE: Ragas is best suited for finding *directional* changes in your LLM-based systems. The absolute scores aren't comparable in a vacuum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Preparation\n",
        "\n",
        "We'll prepare our data - and download our webpages which we'll be using for our data today.\n",
        "\n",
        "These webpages are from [Simon Willison's](https://simonwillison.net/) yearly \"AI learnings\".\n",
        "\n",
        "- [2023 Blog](https://simonwillison.net/2023/Dec/31/ai-in-2023/)\n",
        "- [2024 Blog](https://simonwillison.net/2024/Dec/31/llms-in-2024/)\n",
        "\n",
        "Let's start by collecting our data into a useful pile!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 31287    0 31287    0     0  92995      0 --:--:-- --:--:-- --:--:-- 92839\n"
          ]
        }
      ],
      "source": [
        "!curl https://simonwillison.net/2023/Dec/31/ai-in-2023/ -o data/2023_llms.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 70146    0 70146    0     0   182k      0 --:--:-- --:--:-- --:--:--  183k\n"
          ]
        }
      ],
      "source": [
        "!curl https://simonwillison.net/2024/Dec/31/llms-in-2024/ -o data/2024_llms.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, let's load our data into a familiar LangChain format using the `DirectoryLoader`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
            "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader\n",
        "\n",
        "path = \"data/\"\n",
        "loader = DirectoryLoader(path, glob=\"*.html\")\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Knowledge Graph Based Synthetic Generation\n",
        "\n",
        "Ragas uses a knowledge graph based approach to create data. This is extremely useful as it allows us to create complex queries rather simply. The additional testset complexity allows us to evaluate larger problems more effectively, as systems tend to be very strong on simple evaluation tasks.\n",
        "\n",
        "Let's start by defining our `generator_llm` (which will generate our questions, summaries, and more), and our `generator_embeddings` which will be useful in building our graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Unrolled SDG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-nano\"))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we're going to instantiate our Knowledge Graph.\n",
        "\n",
        "This graph will contain N number of nodes that have M number of relationships. These nodes and relationships (AKA \"edges\") will define our knowledge graph and be used later to construct relevant questions and responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KnowledgeGraph(nodes: 0, relationships: 0)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ragas.testset.graph import KnowledgeGraph\n",
        "\n",
        "kg = KnowledgeGraph()\n",
        "kg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first step we're going to take is to simply insert each of our full documents into the graph. This will provide a base that we can apply transformations to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KnowledgeGraph(nodes: 2, relationships: 0)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ragas.testset.graph import Node, NodeType\n",
        "\n",
        "for doc in docs:\n",
        "    kg.nodes.append(\n",
        "        Node(\n",
        "            type=NodeType.DOCUMENT,\n",
        "            properties={\"page_content\": doc.page_content, \"document_metadata\": doc.metadata}\n",
        "        )\n",
        "    )\n",
        "kg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we'll apply the *default* transformations to our knowledge graph. This will take the nodes currently on the graph and transform them based on a set of [default transformations](https://docs.ragas.io/en/latest/references/transforms/#ragas.testset.transforms.default_transforms).\n",
        "\n",
        "These default transformations are dependent on the corpus length, in our case:\n",
        "\n",
        "- Producing Summaries -> produces summaries of the documents\n",
        "- Extracting Headlines -> finding the overall headline for the document\n",
        "- Theme Extractor -> extracts broad themes about the documents\n",
        "\n",
        "It then uses cosine-similarity and heuristics between the embeddings of the above transformations to construct relationships between the nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "96e473ddab23484ab049df99cf9305e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlinesExtractor:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "317aea4890f1464b8752d35d928f071a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlineSplitter:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d93c6c8674fb477bb0a671ff71929827",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55291aec516f4abdbcadfdc21d8087e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60944b22789d428f8db74ebf17005578",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "91698c3355e549abbafea315ec2a8ce2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "KnowledgeGraph(nodes: 11, relationships: 37)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from ragas.testset.transforms import default_transforms, apply_transforms\n",
        "\n",
        "transformer_llm = generator_llm\n",
        "embedding_model = generator_embeddings\n",
        "\n",
        "default_transforms = default_transforms(documents=docs, llm=transformer_llm, embedding_model=embedding_model)\n",
        "apply_transforms(kg, default_transforms)\n",
        "kg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can save and load our knowledge graphs as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KnowledgeGraph(nodes: 11, relationships: 37)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "kg.save(\"ai_across_years_kg.json\")\n",
        "ai_across_years_kg = KnowledgeGraph.load(\"ai_across_years_kg.json\")\n",
        "ai_across_years_kg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using our knowledge graph, we can construct a \"test set generator\" - which will allow us to create queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=embedding_model, knowledge_graph=ai_across_years_kg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, we'd like to be able to define the kinds of queries we're generating - which is made simple by Ragas having pre-created a number of different \"QuerySynthesizer\"s.\n",
        "\n",
        "Each of these Synthetsizers is going to tackle a separate kind of query which will be generated from a scenario and a persona.\n",
        "\n",
        "In essence, Ragas will use an LLM to generate a persona of someone who would interact with the data - and then use a scenario to construct a question from that data and persona."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.testset.synthesizers import default_query_distribution, SingleHopSpecificQuerySynthesizer, MultiHopAbstractQuerySynthesizer, MultiHopSpecificQuerySynthesizer\n",
        "\n",
        "query_distribution = [\n",
        "        (SingleHopSpecificQuerySynthesizer(llm=generator_llm), 0.5),\n",
        "        (MultiHopAbstractQuerySynthesizer(llm=generator_llm), 0.25),\n",
        "        (MultiHopSpecificQuerySynthesizer(llm=generator_llm), 0.25),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ❓ Question #1:\n",
        "\n",
        "What are the three types of query synthesizers doing? Describe each one in simple terms.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 📍 Answer #1\n",
        "\n",
        "<div style=\"color: blue; font-family: Arial, sans-serif;\">\n",
        "\n",
        "**SingleHopSpecificQuerySynthesizer:** Creates direct questions that can be answered from a single piece of information in the docs. These are focused questions that typically ask about specific facts, definitions, or details contained in one location in the text\n",
        "\n",
        "**Examples:**\n",
        "- \"What is midjourney?\"\n",
        "- \"Why is 2023 considered a breakthrough year for AI?\"\n",
        "- \"What is the significance of Abu Dhabi in the context of AI?\"\n",
        "\n",
        "**MultiHopAbstractQuerySynthesizer:** Generates broader, conceptual questions that require synthesizing information from multiple parts of the loaded docs. These questions are more theoretical and ask about relationships, impacts, or high-level concepts that span across different sections of content.\n",
        "\n",
        "**Examples:**\n",
        "- \"How does the environmental impact of AI training affect the future development of models?\"\n",
        "- \"How can prompt engineering influence the effectiveness of language models?\"\n",
        "- \"How large language models (LLMs) are related to future AI developments?\"\n",
        "\n",
        "**MultiHopSpecificQuerySynthesizer:** Creates complex questions that require connecting specific details from different parts of the loaded docs. These combine the specificity of single-hop queries with the multi-source nature of abstract queries, asking about detailed relationships between specific entities or concepts.\n",
        "\n",
        "**Examples:**\n",
        "- \"How does Llama 3.1 relate to the development of open-source AI models?\"\n",
        "- \"What is Claude's role in integrating AI features across different platforms?\"\n",
        "- \"How do LLMs impact AI development and how are they so advanced?\"\n",
        "\n",
        "Each synthesizer is given a weight in the distribution (0.5, 0.25, 0.25) to control how many of each type appear in the generated test dataset, creating a balanced mix of question complexity to better evaluate RAG systems\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can use our `TestSetGenerator` to generate our testset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f85c63bb5d24e4fb7c7273e2e730d45",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "310353b08db54a52a77f068798512db4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac2dad0a43c94e6a88632e3657b222ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/11 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What are LLMs and why are they significant?</td>\n",
              "      <td>[The ethics of this space remain diabolically ...</td>\n",
              "      <td>Large Language Models (LLMs) are software syst...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Who Simon Willison</td>\n",
              "      <td>[Simon Willison’s Weblog Subscribe Stuff we fi...</td>\n",
              "      <td>Simon Willison’s Weblog discusses that 2023 wa...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What are LLMs and why are they significant in ...</td>\n",
              "      <td>[the document includes some of the clearest ex...</td>\n",
              "      <td>Large language models (LLMs) are a central top...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>As an AI research scientist, how does OpenAI's...</td>\n",
              "      <td>[The year of slop Synthetic training data work...</td>\n",
              "      <td>The context indicates that in 2024, OpenAI is ...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How does ChatGPT's voice mode enhance user int...</td>\n",
              "      <td>[also pre-announced voice mode for Amazon Nova...</td>\n",
              "      <td>ChatGPT voice mode now provides the option to ...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How do AI capabilities like question answering...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nThe ethics of this space remain di...</td>\n",
              "      <td>Recent developments have shown that large lang...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>how AI training impact environment and is it b...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nI would be very surprised if they ...</td>\n",
              "      <td>The context explains that the energy usage and...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How access to AI models and their avilability ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nalso pre-announced voice mode for ...</td>\n",
              "      <td>The context explains that for a few short mont...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>How do Llama 3.1 and Llama 3.3 70B models comp...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nthe importance of synthetic data. ...</td>\n",
              "      <td>The context indicates that Meta's Llama 3.3 70...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Hw do LLMs work and why are they so easy to bu...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nThe ethics of this space remain di...</td>\n",
              "      <td>The document explains that LLMs are built usin...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>so tell me how LLMs in 2024 like what Simon Wi...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willison’s Weblog Subscribe ...</td>\n",
              "      <td>Simon Willison’s weblog in 2024 discusses a lo...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           user_input  \\\n",
              "0         What are LLMs and why are they significant?   \n",
              "1                                  Who Simon Willison   \n",
              "2   What are LLMs and why are they significant in ...   \n",
              "3   As an AI research scientist, how does OpenAI's...   \n",
              "4   How does ChatGPT's voice mode enhance user int...   \n",
              "5   How do AI capabilities like question answering...   \n",
              "6   how AI training impact environment and is it b...   \n",
              "7   How access to AI models and their avilability ...   \n",
              "8   How do Llama 3.1 and Llama 3.3 70B models comp...   \n",
              "9   Hw do LLMs work and why are they so easy to bu...   \n",
              "10  so tell me how LLMs in 2024 like what Simon Wi...   \n",
              "\n",
              "                                   reference_contexts  \\\n",
              "0   [The ethics of this space remain diabolically ...   \n",
              "1   [Simon Willison’s Weblog Subscribe Stuff we fi...   \n",
              "2   [the document includes some of the clearest ex...   \n",
              "3   [The year of slop Synthetic training data work...   \n",
              "4   [also pre-announced voice mode for Amazon Nova...   \n",
              "5   [<1-hop>\\n\\nThe ethics of this space remain di...   \n",
              "6   [<1-hop>\\n\\nI would be very surprised if they ...   \n",
              "7   [<1-hop>\\n\\nalso pre-announced voice mode for ...   \n",
              "8   [<1-hop>\\n\\nthe importance of synthetic data. ...   \n",
              "9   [<1-hop>\\n\\nThe ethics of this space remain di...   \n",
              "10  [<1-hop>\\n\\nSimon Willison’s Weblog Subscribe ...   \n",
              "\n",
              "                                            reference  \\\n",
              "0   Large Language Models (LLMs) are software syst...   \n",
              "1   Simon Willison’s Weblog discusses that 2023 wa...   \n",
              "2   Large language models (LLMs) are a central top...   \n",
              "3   The context indicates that in 2024, OpenAI is ...   \n",
              "4   ChatGPT voice mode now provides the option to ...   \n",
              "5   Recent developments have shown that large lang...   \n",
              "6   The context explains that the energy usage and...   \n",
              "7   The context explains that for a few short mont...   \n",
              "8   The context indicates that Meta's Llama 3.3 70...   \n",
              "9   The document explains that LLMs are built usin...   \n",
              "10  Simon Willison’s weblog in 2024 discusses a lo...   \n",
              "\n",
              "                        synthesizer_name  \n",
              "0   single_hop_specifc_query_synthesizer  \n",
              "1   single_hop_specifc_query_synthesizer  \n",
              "2   single_hop_specifc_query_synthesizer  \n",
              "3   single_hop_specifc_query_synthesizer  \n",
              "4   single_hop_specifc_query_synthesizer  \n",
              "5   multi_hop_abstract_query_synthesizer  \n",
              "6   multi_hop_abstract_query_synthesizer  \n",
              "7   multi_hop_abstract_query_synthesizer  \n",
              "8   multi_hop_specific_query_synthesizer  \n",
              "9   multi_hop_specific_query_synthesizer  \n",
              "10  multi_hop_specific_query_synthesizer  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "testset = generator.generate(testset_size=10, query_distribution=query_distribution)\n",
        "testset.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Abstracted SDG\n",
        "\n",
        "The above method is the full process - but we can shortcut that using the provided abstractions!\n",
        "\n",
        "This will generate our knowledge graph under the hood, and will - from there - generate our personas and scenarios to construct our queries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a82bb876b3e6438c93d8c98e185228ea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlinesExtractor:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1bb2fb6f50e849a3a421b76fa9cf4c21",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlineSplitter:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "020a8650be384670adc9c86d8b61911a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d765e51b2304478e87cc40ca2ebbbcbb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b97be6f11594ab5914fab981684f091",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/26 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a5ae04bd5f64666a29592c6d6a4e522",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "511bc0a90d504909bd44e6abaae132af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ddcedf08f12406b8f29d7f9ce2dcfdc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd83e89c3e59408386c92cab60b76c04",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from ragas.testset import TestsetGenerator\n",
        "\n",
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "dataset = generator.generate_with_langchain_docs(docs, testset_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is TII and how has it contributed to the ...</td>\n",
              "      <td>[The ethics of this space remain diabolically ...</td>\n",
              "      <td>TII, based in Abu Dhabi, is one of the organiz...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What is the relationship between Microsoft and...</td>\n",
              "      <td>[and software engineer, LLMs are infuriating. ...</td>\n",
              "      <td>The provided context does not specify the rela...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Considering Simon Willison’s insights from his...</td>\n",
              "      <td>[Simon Willison’s Weblog Subscribe Stuff we fi...</td>\n",
              "      <td>Simon Willison’s weblog highlights that 2023 w...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What role does OpenAI play in the development ...</td>\n",
              "      <td>[the document includes some of the clearest ex...</td>\n",
              "      <td>The context mentions OpenAI as one of the key ...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How do the increased context lengths and input...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nThe rise of inference-scaling “rea...</td>\n",
              "      <td>The Phi-4 technical report emphasizes that syn...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>how can prompt engineering and injection be us...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nOpenAI aren’t the only group with ...</td>\n",
              "      <td>The context describes how recent advancements ...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>How do misconceptions about AI and the gullibi...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nagain, on the challenge of gullibi...</td>\n",
              "      <td>The context highlights that LLMs tend to belie...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How do the recent developments in multi-modal ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\non a story about the town's histor...</td>\n",
              "      <td>Recent advancements in multi-modal AI models, ...</td>\n",
              "      <td>multi_hop_abstract_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>How do the complex legal and ethical considera...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nthe document includes some of the ...</td>\n",
              "      <td>The context highlights that the legal argument...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>How do Apple’s MLX library and Apple Intellige...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nSimon Willison’s Weblog Subscribe ...</td>\n",
              "      <td>According to the context, Apple’s MLX library ...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>How do the developments in Llama 2 and Llama 3...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nthe document includes some of the ...</td>\n",
              "      <td>The context highlights significant progress in...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>How do the themes of Llama and Llama 2 relate ...</td>\n",
              "      <td>[&lt;1-hop&gt;\\n\\nthe document includes some of the ...</td>\n",
              "      <td>The context segments highlight significant pro...</td>\n",
              "      <td>multi_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           user_input  \\\n",
              "0   What is TII and how has it contributed to the ...   \n",
              "1   What is the relationship between Microsoft and...   \n",
              "2   Considering Simon Willison’s insights from his...   \n",
              "3   What role does OpenAI play in the development ...   \n",
              "4   How do the increased context lengths and input...   \n",
              "5   how can prompt engineering and injection be us...   \n",
              "6   How do misconceptions about AI and the gullibi...   \n",
              "7   How do the recent developments in multi-modal ...   \n",
              "8   How do the complex legal and ethical considera...   \n",
              "9   How do Apple’s MLX library and Apple Intellige...   \n",
              "10  How do the developments in Llama 2 and Llama 3...   \n",
              "11  How do the themes of Llama and Llama 2 relate ...   \n",
              "\n",
              "                                   reference_contexts  \\\n",
              "0   [The ethics of this space remain diabolically ...   \n",
              "1   [and software engineer, LLMs are infuriating. ...   \n",
              "2   [Simon Willison’s Weblog Subscribe Stuff we fi...   \n",
              "3   [the document includes some of the clearest ex...   \n",
              "4   [<1-hop>\\n\\nThe rise of inference-scaling “rea...   \n",
              "5   [<1-hop>\\n\\nOpenAI aren’t the only group with ...   \n",
              "6   [<1-hop>\\n\\nagain, on the challenge of gullibi...   \n",
              "7   [<1-hop>\\n\\non a story about the town's histor...   \n",
              "8   [<1-hop>\\n\\nthe document includes some of the ...   \n",
              "9   [<1-hop>\\n\\nSimon Willison’s Weblog Subscribe ...   \n",
              "10  [<1-hop>\\n\\nthe document includes some of the ...   \n",
              "11  [<1-hop>\\n\\nthe document includes some of the ...   \n",
              "\n",
              "                                            reference  \\\n",
              "0   TII, based in Abu Dhabi, is one of the organiz...   \n",
              "1   The provided context does not specify the rela...   \n",
              "2   Simon Willison’s weblog highlights that 2023 w...   \n",
              "3   The context mentions OpenAI as one of the key ...   \n",
              "4   The Phi-4 technical report emphasizes that syn...   \n",
              "5   The context describes how recent advancements ...   \n",
              "6   The context highlights that LLMs tend to belie...   \n",
              "7   Recent advancements in multi-modal AI models, ...   \n",
              "8   The context highlights that the legal argument...   \n",
              "9   According to the context, Apple’s MLX library ...   \n",
              "10  The context highlights significant progress in...   \n",
              "11  The context segments highlight significant pro...   \n",
              "\n",
              "                        synthesizer_name  \n",
              "0   single_hop_specifc_query_synthesizer  \n",
              "1   single_hop_specifc_query_synthesizer  \n",
              "2   single_hop_specifc_query_synthesizer  \n",
              "3   single_hop_specifc_query_synthesizer  \n",
              "4   multi_hop_abstract_query_synthesizer  \n",
              "5   multi_hop_abstract_query_synthesizer  \n",
              "6   multi_hop_abstract_query_synthesizer  \n",
              "7   multi_hop_abstract_query_synthesizer  \n",
              "8   multi_hop_specific_query_synthesizer  \n",
              "9   multi_hop_specific_query_synthesizer  \n",
              "10  multi_hop_specific_query_synthesizer  \n",
              "11  multi_hop_specific_query_synthesizer  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vSRr2MXk0P_"
      },
      "source": [
        "We'll need to provide our LangSmith API key, and set tracing to \"true\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLDUsLJg43k7"
      },
      "source": [
        "# 🤝 BREAKOUT ROOM #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SLtk1GtnyoY"
      },
      "source": [
        "## Task 4: LangSmith Dataset\n",
        "\n",
        "Now we can move on to creating a dataset for LangSmith!\n",
        "\n",
        "First, we'll need to create a dataset on LangSmith using the `Client`!\n",
        "\n",
        "We'll name our Dataset to make it easy to work with later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "TLgm6OjvYSsm"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "\n",
        "dataset_name = \"State of AI Across the Years - Part Deux!\"\n",
        "\n",
        "langsmith_dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"State of AI Across the Years - Part Deux!\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64SmXMBnzXWm"
      },
      "source": [
        "We'll iterate through the RAGAS created dataframe - and add each example to our created dataset!\n",
        "\n",
        "> NOTE: We need to conform the outputs to the expected format - which in this case is: `question` and `answer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8nFQ6di_XnY7"
      },
      "outputs": [],
      "source": [
        "for data_row in dataset.to_pandas().iterrows():\n",
        "  client.create_example(\n",
        "      inputs={\n",
        "          \"question\": data_row[1][\"user_input\"]\n",
        "      },\n",
        "      outputs={\n",
        "          \"answer\": data_row[1][\"reference\"]\n",
        "      },\n",
        "      metadata={\n",
        "          \"context\": data_row[1][\"reference_contexts\"]\n",
        "      },\n",
        "      dataset_id=langsmith_dataset.id\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6EbQVyZq-2j"
      },
      "source": [
        "## Basic RAG Chain\n",
        "\n",
        "Time for some RAG!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "4njbUAIsaYjB"
      },
      "outputs": [],
      "source": [
        "rag_documents = docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQorBy8H1AZR"
      },
      "source": [
        "To keep things simple, we'll just use LangChain's recursive character text splitter!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qWo3Ajaragv1"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 50\n",
        ")\n",
        "\n",
        "rag_documents = text_splitter.split_documents(rag_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kghuTb9R01oO"
      },
      "source": [
        "We'll create our vectorstore using OpenAI's [`text-embedding-3-small`](https://platform.openai.com/docs/guides/embeddings/embedding-models) embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "UwfJCzP3aqKI"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpCLS-a01Ft2"
      },
      "source": [
        "As usual, we will power our RAG application with Qdrant!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "58Ypj_NgbEsi"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    documents=rag_documents,\n",
        "    embedding=embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"State of AI - Part Deux!\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "SbKSjfSkbTYo"
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxUOMaQX1K2N"
      },
      "source": [
        "To get the \"A\" in RAG, we'll provide a prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "1sLeY1oWbVqO"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT = \"\"\"\\\n",
        "Given a provided context and question, you must answer the question based only on context.\n",
        "\n",
        "If you cannot answer the question based on the context - you must say \"I don't know\".\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZnHDh4e1Ou5"
      },
      "source": [
        "For our LLM, we will be using TogetherAI's endpoints as well!\n",
        "\n",
        "We're going to be using Meta Llama 3.1 70B Instruct Turbo - a powerful model which should get us powerful results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "6nx-ue1XbciV"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmTL6-pc1ZGz"
      },
      "source": [
        "Finally, we can set-up our RAG LCEL chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "TjWj0OLIbbFc"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "    | rag_prompt | llm | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WQ7bEweo4IIb",
        "outputId": "d161b269-f799-4920-d6ce-c202f6e783aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, \"agents\" is an infuriatingly vague term with no single, clear, and widely understood meaning. Generally, there are two main interpretations:\\n\\n1. AI systems that go and act on your behalf, similar to a travel agent model.\\n2. Large language models (LLMs) given access to tools which they can use iteratively to solve problems.\\n\\nThe term is often associated with \"autonomy,\" but without a clear definition. Despite being widely discussed and anticipated, agents have not yet been realized in meaningful, production-level applications. They remain a concept that is perpetually \"coming soon.\"\\n\\nAdditionally, the usefulness of agents is questioned due to the challenge of gullibility—LLMs tend to believe whatever they are told, which limits an agent\\'s ability to make reliable decisions on behalf of a user. Fully solving gullibility may require artificial general intelligence (AGI), making true autonomous agents a distant goal.\\n\\nIn summary, \"agents\" refers to AI systems intended to act autonomously on a user\\'s behalf, but the term lacks precise definition, and such agents have not yet materialized effectively.'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_chain.invoke({\"question\" : \"What are Agents?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9hBh5YPrdGJ"
      },
      "source": [
        "## LangSmith Evaluation Set-up\n",
        "\n",
        "We'll use OpenAI's GPT-4.1 as our evaluation LLM for our base Evaluators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "gfwPYdIkcvpF"
      },
      "outputs": [],
      "source": [
        "eval_llm = ChatOpenAI(model=\"gpt-4.1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b8pToKH2K28"
      },
      "source": [
        "We'll be using a number of evaluators - from LangSmith provided evaluators, to a few custom evaluators!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "PXSG-_ajckp6"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
        "\n",
        "qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\" : eval_llm})\n",
        "\n",
        "labeled_helpfulness_evaluator = LangChainStringEvaluator(\n",
        "    \"labeled_criteria\",\n",
        "    config={\n",
        "        \"criteria\": {\n",
        "            \"helpfulness\": (\n",
        "                \"Is this submission helpful to the user,\"\n",
        "                \" taking into account the correct reference answer?\"\n",
        "            )\n",
        "        },\n",
        "        \"llm\" : eval_llm\n",
        "    },\n",
        "    prepare_data=lambda run, example: {\n",
        "        \"prediction\": run.outputs[\"output\"],\n",
        "        \"reference\": example.outputs[\"answer\"],\n",
        "        \"input\": example.inputs[\"question\"],\n",
        "    }\n",
        ")\n",
        "\n",
        "dope_or_nope_evaluator = LangChainStringEvaluator(\n",
        "    \"criteria\",\n",
        "    config={\n",
        "        \"criteria\": {\n",
        "            \"dopeness\": \"Is this submission dope, lit, or cool?\",\n",
        "        },\n",
        "        \"llm\" : eval_llm\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0SQP_FoCetP"
      },
      "source": [
        "#### 🏗️ Activity #2:\n",
        "\n",
        "Highlight what each evaluator is evaluating.\n",
        "\n",
        "- `qa_evaluator`:\n",
        "- `labeled_helpfulness_evaluator`:\n",
        "- `dope_or_nope_evaluator`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 💡 Solution Activity #2:\n",
        "\n",
        "<div style=\"color: blue; font-family:sans-serif;\">\n",
        "The three evaluators are assessing different aspects of LLM responses:\n",
        "<ul>\n",
        "<li> <b> qa_evaluator:</b> Evaluates the correctness of answers by comparing them against reference answers. This built-in evaluator measures factual accuracy and completeness of responses to questions.</li>\n",
        "<li> <b> labeled_helpfulness_evaluator:</b> Assesses how helpful the response is to the user, considering the reference answer as ground truth. The custom prepare_data function maps the model's output, the expected answer, and the original question for proper evaluation.</li>\n",
        "<li> <b> dope_or_nope_evaluator:</b> A subjective evaluator that measures the \"coolness\" or stylistic appeal of responses - whether they're engaging, interesting, or appealing in tone. This is a less standard metric focused on user experience rather than factual correctness.\n",
        "</li>\n",
        "</ul>\n",
        "Each evaluator uses the same LLM (eval_llm) to perform these assessments, leveraging it as a judge of the generated content.\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R35sQMHVrnpl"
      },
      "source": [
        "## LangSmith Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "122b1bd1f0e9417a8dcb57d4eebe4d2e",
            "e0c233ad01604540a6c873f4a731982d",
            "e9a01115c75b499884f7e0ef32e9e599",
            "5faba4ad609448b2b49024add4ad3b8e",
            "ef25efa751304e4699910f1fbc14345f",
            "0b44cb0f8e34446c8dde668a75d3d8ad",
            "edaac6587b2d4bd5be52b89bb097f99f",
            "7cb241365f604419af454c1c28de197a",
            "9cf586576ff44dba86ba2eb389593c61",
            "849b5c95008541d49f1ceedf0a59ac60",
            "f3665a86662746c4ac7cb0796604781d"
          ]
        },
        "id": "t7t_Uz0tdumL",
        "outputId": "d684e218-294e-4dc3-c8de-a01d397f021c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'pertinent-sheet-64' at:\n",
            "https://smith.langchain.com/o/897469e2-b134-4dc6-8475-ea3514973522/datasets/e03314aa-7a19-442d-8e99-30223e07367c/compare?selectedSessions=c0e505fb-fbbd-4a7b-a569-43748fb948e0\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9b3bbd6df0e405cb3c108716f27be92",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inputs.question</th>\n",
              "      <th>outputs.output</th>\n",
              "      <th>error</th>\n",
              "      <th>reference.answer</th>\n",
              "      <th>feedback.correctness</th>\n",
              "      <th>feedback.helpfulness</th>\n",
              "      <th>feedback.dopeness</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>example_id</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How do the themes of Llama and Llama 2 relate ...</td>\n",
              "      <td>The themes of Llama and Llama 2 relate to the ...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context segments highlight significant pro...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4.578360</td>\n",
              "      <td>72d75f4b-a825-402a-b663-927f8a46472c</td>\n",
              "      <td>a65d3017-384d-4bd4-8ee4-1fd981d46d2f</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How do the developments in Llama 2 and Llama 3...</td>\n",
              "      <td>The developments in Llama 2 and Llama 3.1 illu...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context highlights significant progress in...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>8.290508</td>\n",
              "      <td>379a8e71-a03e-4b81-8256-a054de17230f</td>\n",
              "      <td>cfbda48f-1d38-45ea-bb76-accd8700577a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How do Apple’s MLX library and Apple Intellige...</td>\n",
              "      <td>Based on the provided context, Apple’s MLX lib...</td>\n",
              "      <td>None</td>\n",
              "      <td>According to the context, Apple’s MLX library ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6.865152</td>\n",
              "      <td>ab12d552-c1fe-43e3-962e-bf1e92d1b1d2</td>\n",
              "      <td>324ada45-0551-48c4-a7b1-bbdeef4174e1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How do the complex legal and ethical considera...</td>\n",
              "      <td>The context indicates that the legal arguments...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context highlights that the legal argument...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3.703110</td>\n",
              "      <td>ed06d0b9-6022-438e-8d74-d07dcc2654e6</td>\n",
              "      <td>b4e231f0-2d98-44d3-93c5-5067b0e077f0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How do the recent developments in multi-modal ...</td>\n",
              "      <td>Recent developments in multi-modal AI models h...</td>\n",
              "      <td>None</td>\n",
              "      <td>Recent advancements in multi-modal AI models, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6.114835</td>\n",
              "      <td>50097dc4-1f7e-4999-b867-69404c8d2c3d</td>\n",
              "      <td>fe9ea50d-5e30-4efa-8bd5-768266267d0a</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How do misconceptions about AI and the gullibi...</td>\n",
              "      <td>Based on the provided context, misconceptions ...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context highlights that LLMs tend to belie...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5.633590</td>\n",
              "      <td>e2fabfe2-67da-4966-a56a-a1ef6c2db39b</td>\n",
              "      <td>e6722da8-17f6-456e-9bcb-2fa8e269da80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>how can prompt engineering and injection be us...</td>\n",
              "      <td>I don't know.</td>\n",
              "      <td>None</td>\n",
              "      <td>The context describes how recent advancements ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.393685</td>\n",
              "      <td>5e4ae7a5-7ac5-4d6c-a2e4-1d684d7feaa1</td>\n",
              "      <td>cb2ad1c4-7377-46fb-aa7f-57efc03eedda</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How do the increased context lengths and input...</td>\n",
              "      <td>Based on the provided context, increased conte...</td>\n",
              "      <td>None</td>\n",
              "      <td>The Phi-4 technical report emphasizes that syn...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5.113304</td>\n",
              "      <td>8e81c5ab-772f-41e5-921b-eab6db274e89</td>\n",
              "      <td>20397119-d859-4964-bde3-db6bf8ea37d8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>What role does OpenAI play in the development ...</td>\n",
              "      <td>OpenAI played a pioneering role in the develop...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context mentions OpenAI as one of the key ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2.797491</td>\n",
              "      <td>41768a60-f87b-4c19-b3e4-5827abbfac65</td>\n",
              "      <td>dcab337b-2c0d-49f2-a1ab-64839e6ddabe</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Considering Simon Willison’s insights from his...</td>\n",
              "      <td>Based on the provided context from Simon Willi...</td>\n",
              "      <td>None</td>\n",
              "      <td>Simon Willison’s weblog highlights that 2023 w...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>6.043943</td>\n",
              "      <td>d7b9f92a-fdee-4a84-933d-fe419d723114</td>\n",
              "      <td>7e532c74-75d6-4d90-b172-0794b5b4af7d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>What is the relationship between Microsoft and...</td>\n",
              "      <td>Based on the provided context, Microsoft is in...</td>\n",
              "      <td>None</td>\n",
              "      <td>The provided context does not specify the rela...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2.733308</td>\n",
              "      <td>2306b155-b1ee-404c-8c31-048a3ae25acb</td>\n",
              "      <td>85fd6d66-ba0c-4e21-b3d3-3c13cd195e67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>What is TII and how has it contributed to the ...</td>\n",
              "      <td>Based on the provided context, TII refers to a...</td>\n",
              "      <td>None</td>\n",
              "      <td>TII, based in Abu Dhabi, is one of the organiz...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2.147537</td>\n",
              "      <td>bc34f811-a66e-4551-9207-623c23c3ef24</td>\n",
              "      <td>d2910e37-a671-4f5a-a5ef-9b90a801157d</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "<ExperimentResults pertinent-sheet-64>"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(\n",
        "    rag_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator,\n",
        "        labeled_helpfulness_evaluator,\n",
        "        dope_or_nope_evaluator\n",
        "    ],\n",
        "    metadata={\"revision_id\": \"default_chain_init\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq7fCVinrpI4"
      },
      "source": [
        "## Dope-ifying Our Application\n",
        "\n",
        "We'll be making a few changes to our RAG chain to increase its performance on our SDG evaluation test dataset!\n",
        "\n",
        "- Include a \"dope\" prompt augmentation\n",
        "- Use larger chunks\n",
        "- Improve the retriever model to: `text-embedding-3-large`\n",
        "\n",
        "Let's see how this changes our evaluation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "z56pXwyUgFUt"
      },
      "outputs": [],
      "source": [
        "DOPE_RAG_PROMPT = \"\"\"\\\n",
        "Given a provided context and question, you must answer the question based only on context.\n",
        "\n",
        "If you cannot answer the question based on the context - you must say \"I don't know\".\n",
        "\n",
        "You must answer the questions in a dope way, be cool!\n",
        "\n",
        "Context: {context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "dope_rag_prompt = ChatPromptTemplate.from_template(DOPE_RAG_PROMPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "rZLcTstJgfv5"
      },
      "outputs": [],
      "source": [
        "rag_documents = docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "-LYsyirngj6n"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1000,\n",
        "    chunk_overlap = 50\n",
        ")\n",
        "\n",
        "rag_documents = text_splitter.split_documents(rag_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spldiPuTCzDO"
      },
      "source": [
        "#### ❓Question #2:\n",
        "\n",
        "Why would modifying our chunk size modify the performance of our application?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 📍 Answer #2:\n",
        "<div style=\"color: blue; font-family:sans-serif;\">\n",
        "\n",
        "Increasing the chunk size from 500 to 1000 characters likely improves performance by providing more complete context to the retrieval system, which helps the model generate more accurate and comprehensive responses, especially for complex or nuanced questions. Smaller chunks provide more focused and precise retrieval that can target specific information, but might miss broader context\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "b9MI2Bm2go1r"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBbjG6cKC8BQ"
      },
      "source": [
        "#### ❓Question #3:\n",
        "\n",
        "Why would modifying our embedding model modify the performance of our application?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 📍 Answer #3:\n",
        "\n",
        "<div style=\"color: blue; font-family:sans-serif;\">\n",
        "\n",
        "Modifying the embedding model from \"text-embedding-3-small\" to \"text-embedding-3-large\" will impact RAG performancein several ways:\n",
        "<ul>\n",
        "<li> Better Semantic Understanding: because the large model offers higher dimension embeddings with more nuanced semantics.</li>\n",
        "<li> Better Retrieval Accuracy: It allows for more precise similarity calculations between queries and doc chunks. Will also reduce false positives </li>\n",
        "<li> Better handling of Contextual Complexity: It can provide more robust connection between related technical terms and improved ability to match context in different ways.</li>\n",
        "<li> Better Contextual Awareness: Beter performance with abstract or complex queries and better handling of ambiguous terms based on surrounding context.</li>\n",
        "<li> Better Language Nuance : Larger embedding models can provide better handling of domain specific language and are more effective with indirect or inferential queries.\n",
        "</ul>\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "hVUY25FKgxXx"
      },
      "outputs": [],
      "source": [
        "vectorstore = Qdrant.from_documents(\n",
        "    documents=rag_documents,\n",
        "    embedding=embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"AI Across Years (Augmented)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Q4TOZNYIg2v1"
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqYGFrnKDB91"
      },
      "source": [
        "Setting up our new and improved DOPE RAG CHAIN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "HqnTqeXMhAdx"
      },
      "outputs": [],
      "source": [
        "dope_rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "    | dope_rag_prompt | llm | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21pTxoqJDI1Y"
      },
      "source": [
        "Let's test it on the same output that we saw before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "OfZZ3MoN3fKv",
        "outputId": "d65722dd-92c2-4e4e-9cca-c42ee6f3f208"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Yo, here’s the lowdown on “agents” straight from the context: \\n\\nAgents are kinda like these AI things that people hype but don't really nail down — the term is super vague. Some folks think of agents as AI that can go off and do stuff for you, like a travel agent booking trips on the fly. Others see agents as large language models (LLMs) hooked up with tools, looping through tasks to solve problems. But here’s the catch: no one’s settled on what “autonomy” really means for these agents.\\n\\nDespite all the buzz, real-deal agents that act independently and reliably haven’t really landed in the wild yet. The major snag? Gullibility. These AIs tend to believe whatever they hear, which makes them shaky when you want them to make smart decisions on your behalf. Serious agency needs something like AGI to handle that trustworthiness, and we’re not quite there yet.\\n\\nSo yeah, agents are these hyped-up, kinda fuzzy AI systems meant to do stuff for you, but right now they’re mostly “coming soon” vibes with a big “but can they really be trusted?” question mark.\\n\\nStay fresh. 😎\""
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dope_rag_chain.invoke({\"question\" : \"what are Agents?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpj7v1inDLnQ"
      },
      "source": [
        "Finally, we can evaluate the new chain on the same test set!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "bf8dcc0895054529af356da401c513f6",
            "7dce19ac55264f2b88a0e4730e55867b",
            "2a0755d4476543feb4a64538e3e37213",
            "158212a630f04cbd884c937f2f60f5c8",
            "11c7f66acc1d45be9517d0addf49331e",
            "ddffd834e09940a4bd3874c3f39b4e21",
            "ef63c3b2d51e452da03cdae5d9b034be",
            "c20b539cd70b4ba99601ad1d69fd9cec",
            "a6d681eeafa44d18b933a4c5dec88382",
            "d1d54ccd56494c4d831f71b416a1f880",
            "530f696feefe499da08c6312047379b2"
          ]
        },
        "id": "Dx11S2b-hIM8",
        "outputId": "d3a3ea78-aa32-4bd2-8c2a-d0d0303695c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'earnest-sheet-85' at:\n",
            "https://smith.langchain.com/o/897469e2-b134-4dc6-8475-ea3514973522/datasets/e03314aa-7a19-442d-8e99-30223e07367c/compare?selectedSessions=bb35e60b-1d75-421d-ade8-09608908d2d5\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b11836f63f948faa8a0e67dda3833e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inputs.question</th>\n",
              "      <th>outputs.output</th>\n",
              "      <th>error</th>\n",
              "      <th>reference.answer</th>\n",
              "      <th>feedback.correctness</th>\n",
              "      <th>feedback.helpfulness</th>\n",
              "      <th>feedback.dopeness</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>example_id</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How do the themes of Llama and Llama 2 relate ...</td>\n",
              "      <td>Ah, here’s the lowdown: Llama and Llama 2 vibe...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context segments highlight significant pro...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5.690520</td>\n",
              "      <td>72d75f4b-a825-402a-b663-927f8a46472c</td>\n",
              "      <td>58379bfe-7e9c-4865-bf0d-c039ff9a4351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How do the developments in Llama 2 and Llama 3...</td>\n",
              "      <td>Yo, here’s the lowdown with Llama 2, Llama 3.1...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context highlights significant progress in...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5.629988</td>\n",
              "      <td>379a8e71-a03e-4b81-8256-a054de17230f</td>\n",
              "      <td>d54bd6d7-a9f8-4fd0-a8f5-ebd040cc449d</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>How do Apple’s MLX library and Apple Intellige...</td>\n",
              "      <td>Yo, here’s the lowdown: Apple’s MLX library is...</td>\n",
              "      <td>None</td>\n",
              "      <td>According to the context, Apple’s MLX library ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7.515837</td>\n",
              "      <td>ab12d552-c1fe-43e3-962e-bf1e92d1b1d2</td>\n",
              "      <td>40f04948-2e22-49ad-a450-dff931b0f116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How do the complex legal and ethical considera...</td>\n",
              "      <td>Yo, here’s the lowdown: The legal and ethical ...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context highlights that the legal argument...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4.617130</td>\n",
              "      <td>ed06d0b9-6022-438e-8d74-d07dcc2654e6</td>\n",
              "      <td>a28fec37-45c5-4bfe-a720-b2768e872787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How do the recent developments in multi-modal ...</td>\n",
              "      <td>Yo, the multi-modal AI wave in 2024 is straigh...</td>\n",
              "      <td>None</td>\n",
              "      <td>Recent advancements in multi-modal AI models, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5.079055</td>\n",
              "      <td>50097dc4-1f7e-4999-b867-69404c8d2c3d</td>\n",
              "      <td>a38b9aaa-6f9d-4f4a-b919-2ee73e83cb97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How do misconceptions about AI and the gullibi...</td>\n",
              "      <td>Alright, here’s the lowdown, straight from the...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context highlights that LLMs tend to belie...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>8.404440</td>\n",
              "      <td>e2fabfe2-67da-4966-a56a-a1ef6c2db39b</td>\n",
              "      <td>8f6ffb68-6422-4cd5-ac85-5dbb547e0730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>how can prompt engineering and injection be us...</td>\n",
              "      <td>Yo, here’s the lowdown on how prompt engineeri...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context describes how recent advancements ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>7.226803</td>\n",
              "      <td>5e4ae7a5-7ac5-4d6c-a2e4-1d684d7feaa1</td>\n",
              "      <td>d870a43d-d652-4088-88e7-5361a095fcac</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How do the increased context lengths and input...</td>\n",
              "      <td>Yo, here’s the lowdown straight from the conte...</td>\n",
              "      <td>None</td>\n",
              "      <td>The Phi-4 technical report emphasizes that syn...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>12.700302</td>\n",
              "      <td>8e81c5ab-772f-41e5-921b-eab6db274e89</td>\n",
              "      <td>9721e15d-5151-43c4-a4bf-6b41593f251f</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>What role does OpenAI play in the development ...</td>\n",
              "      <td>Yo, OpenAI’s the big boss in the LLM game, dro...</td>\n",
              "      <td>None</td>\n",
              "      <td>The context mentions OpenAI as one of the key ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3.374776</td>\n",
              "      <td>41768a60-f87b-4c19-b3e4-5827abbfac65</td>\n",
              "      <td>63866960-f849-48d4-bc87-8f1afa586fb0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Considering Simon Willison’s insights from his...</td>\n",
              "      <td>Yo, here’s the lowdown straight from Simon Wil...</td>\n",
              "      <td>None</td>\n",
              "      <td>Simon Willison’s weblog highlights that 2023 w...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3.685155</td>\n",
              "      <td>d7b9f92a-fdee-4a84-933d-fe419d723114</td>\n",
              "      <td>362db56c-7a11-451c-9f64-a2c98db0d971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>What is the relationship between Microsoft and...</td>\n",
              "      <td>Yo, based on the vibe from the context, Micros...</td>\n",
              "      <td>None</td>\n",
              "      <td>The provided context does not specify the rela...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2.648359</td>\n",
              "      <td>2306b155-b1ee-404c-8c31-048a3ae25acb</td>\n",
              "      <td>dd9c1858-0e4f-4fab-8f80-b6d596dc9327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>What is TII and how has it contributed to the ...</td>\n",
              "      <td>Yo, I gotta keep it real with you—based on the...</td>\n",
              "      <td>None</td>\n",
              "      <td>TII, based in Abu Dhabi, is one of the organiz...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3.298255</td>\n",
              "      <td>bc34f811-a66e-4551-9207-623c23c3ef24</td>\n",
              "      <td>e9f128f0-7393-4fe8-ab60-55c3f6c50699</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "<ExperimentResults earnest-sheet-85>"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(\n",
        "    dope_rag_chain.invoke,\n",
        "    data=dataset_name,\n",
        "    evaluators=[\n",
        "        qa_evaluator,\n",
        "        labeled_helpfulness_evaluator,\n",
        "        dope_or_nope_evaluator\n",
        "    ],\n",
        "    metadata={\"revision_id\": \"dope_chain\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C7migvlDPZT"
      },
      "source": [
        "#### 🏗️ Activity #3:\n",
        "\n",
        "Provide a screenshot of the difference between the two chains, and explain why you believe certain metrics changed in certain ways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 💡 Solution #3\n",
        "\n",
        "The dope_rag_prompt which we added to the dopeness LCEL chain, has resulted in the dopeness factor of the answers evaluating better.\n",
        "\n",
        "<img src = \"images/Langsmith_Experiment.png\" height = \"500\" width=\"900\">"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07ab3dc0790241bbb85a7f488a42ef8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7710c7377cbc4c30b55b28b4bc99e88f",
              "IPY_MODEL_41bdd49fab5f4826959d0d50663ff539",
              "IPY_MODEL_60168d85131d4afc99d55d61ab954ee6"
            ],
            "layout": "IPY_MODEL_9edf898aeeab40dda9b9475395776521"
          }
        },
        "095f680d37a3430fb82d223615662db5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b44cb0f8e34446c8dde668a75d3d8ad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10df31709059484c99f102453d780473": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1160a44dc18e47b0890f70c40eaa7eb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11c7f66acc1d45be9517d0addf49331e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "122b1bd1f0e9417a8dcb57d4eebe4d2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0c233ad01604540a6c873f4a731982d",
              "IPY_MODEL_e9a01115c75b499884f7e0ef32e9e599",
              "IPY_MODEL_5faba4ad609448b2b49024add4ad3b8e"
            ],
            "layout": "IPY_MODEL_ef25efa751304e4699910f1fbc14345f"
          }
        },
        "158212a630f04cbd884c937f2f60f5c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1d54ccd56494c4d831f71b416a1f880",
            "placeholder": "​",
            "style": "IPY_MODEL_530f696feefe499da08c6312047379b2",
            "value": " 20/? [01:43&lt;00:00,  5.25s/it]"
          }
        },
        "23863bc37a8645029934b8c106622c51": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2508d229935744cbb5fc340222e2d660": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a0755d4476543feb4a64538e3e37213": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c20b539cd70b4ba99601ad1d69fd9cec",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a6d681eeafa44d18b933a4c5dec88382",
            "value": 1
          }
        },
        "33f063017b7c4c7fa8cbafc89674350b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6864c81e2bcf459bbaf5acbb36bdfcbe",
              "IPY_MODEL_59d6e269eadf429a924f6f79bc8ba4ba",
              "IPY_MODEL_ca791fc471e34b9da2f9070fc1053c0f"
            ],
            "layout": "IPY_MODEL_8baf0ed3d0f743f294e07f2b5407e820"
          }
        },
        "3a8537e37fc14fd9b16ca0ceee4fede6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41bdd49fab5f4826959d0d50663ff539": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6eb8b2e3262c45248708a2082c366f0a",
            "max": 64,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_095f680d37a3430fb82d223615662db5",
            "value": 64
          }
        },
        "530f696feefe499da08c6312047379b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59d6e269eadf429a924f6f79bc8ba4ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_890e0dd7fa524ceca1e805cb6253ee71",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61b52ff459214129b8f7e6d67b192b78",
            "value": 20
          }
        },
        "5ab5f08afa5841709aedb2f78a52a11c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c2fda99d4204d85b1bf7ad354fd58d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5faba4ad609448b2b49024add4ad3b8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_849b5c95008541d49f1ceedf0a59ac60",
            "placeholder": "​",
            "style": "IPY_MODEL_f3665a86662746c4ac7cb0796604781d",
            "value": " 20/? [01:27&lt;00:00,  6.45s/it]"
          }
        },
        "60168d85131d4afc99d55d61ab954ee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a8537e37fc14fd9b16ca0ceee4fede6",
            "placeholder": "​",
            "style": "IPY_MODEL_1160a44dc18e47b0890f70c40eaa7eb0",
            "value": " 61/64 [00:02&lt;00:00, 23.36it/s]"
          }
        },
        "61b52ff459214129b8f7e6d67b192b78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6864c81e2bcf459bbaf5acbb36bdfcbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10df31709059484c99f102453d780473",
            "placeholder": "​",
            "style": "IPY_MODEL_2508d229935744cbb5fc340222e2d660",
            "value": "Generating: 100%"
          }
        },
        "6eb8b2e3262c45248708a2082c366f0a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7710c7377cbc4c30b55b28b4bc99e88f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c2fda99d4204d85b1bf7ad354fd58d4",
            "placeholder": "​",
            "style": "IPY_MODEL_93cd4d35c5fd41f5904ca1d52d1f52a8",
            "value": "embedding nodes:  95%"
          }
        },
        "7cb241365f604419af454c1c28de197a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "7dce19ac55264f2b88a0e4730e55867b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ddffd834e09940a4bd3874c3f39b4e21",
            "placeholder": "​",
            "style": "IPY_MODEL_ef63c3b2d51e452da03cdae5d9b034be",
            "value": ""
          }
        },
        "849b5c95008541d49f1ceedf0a59ac60": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "890e0dd7fa524ceca1e805cb6253ee71": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8baf0ed3d0f743f294e07f2b5407e820": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93cd4d35c5fd41f5904ca1d52d1f52a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cf586576ff44dba86ba2eb389593c61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9edf898aeeab40dda9b9475395776521": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "a6d681eeafa44d18b933a4c5dec88382": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bf8dcc0895054529af356da401c513f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7dce19ac55264f2b88a0e4730e55867b",
              "IPY_MODEL_2a0755d4476543feb4a64538e3e37213",
              "IPY_MODEL_158212a630f04cbd884c937f2f60f5c8"
            ],
            "layout": "IPY_MODEL_11c7f66acc1d45be9517d0addf49331e"
          }
        },
        "c20b539cd70b4ba99601ad1d69fd9cec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ca791fc471e34b9da2f9070fc1053c0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23863bc37a8645029934b8c106622c51",
            "placeholder": "​",
            "style": "IPY_MODEL_5ab5f08afa5841709aedb2f78a52a11c",
            "value": " 20/20 [00:52&lt;00:00,  4.50s/it]"
          }
        },
        "d1d54ccd56494c4d831f71b416a1f880": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddffd834e09940a4bd3874c3f39b4e21": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0c233ad01604540a6c873f4a731982d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b44cb0f8e34446c8dde668a75d3d8ad",
            "placeholder": "​",
            "style": "IPY_MODEL_edaac6587b2d4bd5be52b89bb097f99f",
            "value": ""
          }
        },
        "e9a01115c75b499884f7e0ef32e9e599": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cb241365f604419af454c1c28de197a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9cf586576ff44dba86ba2eb389593c61",
            "value": 1
          }
        },
        "edaac6587b2d4bd5be52b89bb097f99f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef25efa751304e4699910f1fbc14345f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef63c3b2d51e452da03cdae5d9b034be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3665a86662746c4ac7cb0796604781d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "state": {}
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
